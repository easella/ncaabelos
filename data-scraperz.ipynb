{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scikit-learn\n!pip install playwright\n!playwright install\n!playwright install-deps\nprint(\"done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T23:15:13.842510Z","iopub.execute_input":"2026-01-14T23:15:13.842785Z","iopub.status.idle":"2026-01-14T23:16:51.906250Z","shell.execute_reply.started":"2026-01-14T23:15:13.842757Z","shell.execute_reply":"2026-01-14T23:16:51.904845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom playwright.async_api import async_playwright\nimport asyncio\nasync def fetch(link):\n    async with async_playwright() as p:\n        browser=await p.chromium.launch(headless=True)\n        page=await browser.new_page()\n        await page.goto(link,wait_until=\"load\")\n        last=None\n        for _ in range(10):\n            await page.wait_for_load_state(\"networkidle\")\n            new=page.url\n            if new==last:\n                break\n            last=new\n        try:\n            await page.wait_for_selector(\"table\")\n        except Exception as e:\n            print(e)\n        html=await page.content()\n        await browser.close()\n        soup=BeautifulSoup(html)\n        return soup\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\n\n\nasync def fetchz(link, context):\n    page = await context.new_page()\n    await page.route(\"**/*\", lambda route: route.abort() if route.request.resource_type in [\"image\", \"stylesheet\", \"font\"] else route.continue_())\n    await page.goto(link, wait_until=\"domcontentloaded\",timeout=0)\n    last = None\n    for _ in range(10):\n        await page.wait_for_load_state(\"networkidle\")\n        new = page.url\n        if new == last:\n            break\n        last = new\n    try:\n        await page.wait_for_selector(\"table\",state=\"attached\")\n    except Exception as e:\n        print(e)\n    element = await page.query_selector(\"table\")\n    html = await element.inner_html()\n    soup = BeautifulSoup(html, \"html.parser\")\n    await page.close()\n    return soup\n\nasync def main(link):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True,args=[\"--disable-gpu\", \"--no-sandbox\"])\n        context = await browser.new_context()\n        result = await fetchz(link,context)\n        await browser.close()\n        return result\ndates={}\nimport nest_asyncio\nnest_asyncio.apply()\ndates=[]\narrw=[]\nfor i in range(2019,2027):\n    if i!=2020:\n        dates.append(i)\nfor s in dates:\n    print(s)\n    uk=int(s)-1\n    s=str(s)\n    nj=s.replace(\".0\",\"\")\n    s=s.replace(\".0\",\"\")\n    url='https://barttorvik.com/teamsheets.php?year='+str(s)\n    print(url)\n    loop = asyncio.get_event_loop()\n    table = loop.run_until_complete(main(url))\n    for tr in table.find_all(\"tr\"):\n        if \"AdjOE\" not in str(tr) and 'absolute error' not in str(tr) and 'RESUME' not in str(tr) and 'KPI' not in str(tr):\n            rowz=[]\n            n=0\n            for td in tr.find_all(\"td\"):\n                if 1==1:\n                    if int(s)<2024 and n==8:\n                        m=1\n                    elif int(s)==2025 and n==5:\n                        m=1\n                    elif int(s)==2025 and n==9:\n                        m=1\n                    elif int(s)==2026 and n==5:\n                        m=1\n                    elif int(s)==2026 and n==9:\n                        m=1\n                    elif n==1:\n                        fd=td.find_all(\"a\")[0].text\n                        fd=str(fd)\n                        if fd=='':\n                            print(tr.find_all(\"td\"))\n                        rowz.append(fd)\n                    else:\n                        rowz.append(td.text)\n                n+=1\n            rowz.append(uk)\n            arrw.append(rowz)\ndfe=pd.DataFrame(arrw)\ndfe.columns=['rk','team','net','kpi','sor','avg','bpi','kp','avgz','q1a','q1','q2','q12','q3','q4','season']\nfor i in range(2018,2027):\n    dfe=dfe[dfe.rk!=i]\ndfe=dfe.replace('\\xa0', ' ', regex=True)\nfor row in dfe.itertuples():\n    if \"   \" in str(row.team):\n        teamh=str(row.team).split(\"   \")[0]\n        dfe.at[row.Index,'team']=teamh\ndfe['team']=dfe['team'].str.replace(\" St.\",\" State\")\ndfe=dfe.replace(\"Albany\",\"Albany (NY)\")\ndfe=dfe.replace(\"BYU\",\"Brigham Young\")\ndfe=dfe.replace(\"Grambling State\",\"Grambling\")\ndfe=dfe.replace(\"VCU\",\"Virginia Commonwealth\")\ndfe=dfe.replace(\"Fairleigh Dickinson\",\"FDU\")\ndfe=dfe.replace(\"SIU Edwardsville\",\"Southern Illinois-Edwardsville\")\ndfe=dfe.replace(\"LIU\",\"Long Island University\")\ndfe=dfe.replace(\"Nebraska Omaha\",\"Omaha\")\ndfe=dfe.replace(\"UMBC\",\"Maryland-Baltimore County\")\ndfe=dfe.replace(\"Miami FL\",\"Miami (FL)\")\ndfe=dfe.replace(\"SMU\",\"Southern Methodist\")\ndfe=dfe.replace(\"Penn\",\"Pennsylvania\")\ndfe=dfe.replace(\"Arkansas Pine Bluff\",\"Arkansas-Pine Bluff\")\ndfe=dfe.replace(\"USC Upstate\",\"South Carolina Upstate\")\ndfe=dfe.replace(\"St. Francis NY\",\"St. Francis (NY)\")\ndfe=dfe.replace(\"UMKC\",\"Kansas City\")\ndfe=dfe.replace(\"Central Connecticut\",\"Central Connecticut State\")\ndfe=dfe.replace(\"Tennessee Martin\",\"Tennessee-Martin\")\ndfe=dfe.replace(\"Saint Francis\",\"Saint Francis (PA)\")\ndfe=dfe.replace(\"Illinois Chicago\",\"Illinois-Chicago\")\ndfe=dfe.replace(\"Bethune Cookman\",\"Bethune-Cookman\")\ndfe=dfe.replace(\"N.C. State\",\"NC State\")\ndfe=dfe.replace(\"Charleston\",\"College of Charleston\")\ndfe=dfe.replace(\"Maryland Eastern Shore\",\"Maryland-Eastern Shore\")\ndfe=dfe.replace(\"Loyola Chicago\",\"Loyola (IL)\")\ndfe=dfe.replace(\"Louisiana Monroe\",\"Louisiana-Monroe\")\ndfe=dfe.replace(\"Southern Miss\",\"Southern Mississippi\")\ndfe=dfe.replace(\"Texas A&M Corpus Chris\",\"Texas A&M-Corpus Christi\")\ndfe=dfe.replace(\"Saint Mary's\",\"Saint Mary's (CA)\")\ndfe=dfe.replace(\"UT Rio Grande Valley\",\"Texas-Rio Grande Valley\")\ndfe=dfe.replace(\"Gardner Webb\",\"Gardner-Webb\")\ndfe=dfe.replace(\"LSU\",\"Louisiana State\")\ndfe=dfe.replace(\"Loyola MD\",\"Loyola (MD)\")\ndfe=dfe.replace(\"UMass Lowell\",\"Massachusetts-Lowell\")\ndfe=dfe.replace(\"UNLV\",\"Nevada-Las Vegas\")\ndfe=dfe.replace(\"USC\",\"Southern California\")\ndfe=dfe.replace(\"Sam Houston State\",\"Sam Houston\")\ndfe=dfe.replace(\"Prairie View A&M\",\"Prairie View\")\ndfe=dfe.replace(\"Miami OH\",\"Miami (OH)\")\ndfe=dfe.replace(\"VMI\",\"Virginia Military Institute\")\ndfe=dfe.replace(\"St. John's\",\"St. John's (NY)\")\ndfe=dfe.replace(\"FIU\",\"Florida International\")\ndfe=dfe.replace(\"Queens\",\"Queens (NC)\")\ndfe=dfe.replace(\"Cal Baptist\",\"California Baptist\")\ndfe=dfe.replace(\"Mount State Mary's\",\"Mount St. Mary's\")\nrez={}\ndfe=dfe.astype({\"net\":float,\"rk\":float,\"kpi\":float,\"sor\":float,\"bpi\":float,\"kp\":float})\nrez={}\nfor i in range(2017,2027):\n    rez[i]={}\nfor row in dfe.itertuples():\n    q1w=float(str(row.q1).split(\"-\")[0])\n    q1l=float(str(row.q1).split(\"-\")[1])\n    q2w=float(str(row.q2).split(\"-\")[0])\n    q2l=float(str(row.q2).split(\"-\")[1])\n    q3w=float(str(row.q3).split(\"-\")[0])\n    q3l=float(str(row.q3).split(\"-\")[1])\n    q4w=float(str(row.q4).split(\"-\")[0])\n    q4l=float(str(row.q4).split(\"-\")[1])\n    q1aw=float(str(row.q1a).split(\"-\")[0])\n    q1al=float(str(row.q1a).split(\"-\")[1])\n    tgz=q1w+q1l+q2w+q2l+q3w+q3l+q4w+q4l\n    w=q1w+q2w+q3w+q4w\n    rez[int(row.season)][row.team]={'resrk':row.rk,'net':row.net,'kpi':row.kpi,'sor':row.sor,'avg':row.avg,'bpi':row.bpi,'kp':row.kp,\n'avgz':row.avgz,'q1w':q1w,'q2w':q2w,'q3w':q3w,'q4w':q4w,'q1l':q1l,'q2l':q2l,'q3l':q3l,'q4l':q4l,'q1aw':q1aw,'q1al':q1al}\narrw=[]\n\nfrom playwright.async_api import async_playwright\nimport asyncio\nfrom sklearn.pipeline import Pipeline\n\nasync def fetch(link):\n    async with async_playwright() as p:\n        browser=await p.chromium.launch(headless=True)\n        page=await browser.new_page()\n        await page.goto(link,wait_until=\"load\")\n        last=None\n        for _ in range(10):\n            await page.wait_for_load_state(\"networkidle\")\n            new=page.url\n            if new==last:\n                break\n            last=new\n        try:\n            await page.wait_for_selector(\"table\")\n        except Exception as e:\n            print(e)\n        html=await page.content()\n        await browser.close()\n        soup=BeautifulSoup(html)\n        return soup\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\n\n\nasync def fetchz(link, context):\n    page = await context.new_page()\n    await page.route(\"**/*\", lambda route: route.abort() if route.request.resource_type in [\"image\", \"stylesheet\", \"font\"] else route.continue_())\n    await page.goto(link, wait_until=\"domcontentloaded\",timeout=0)\n    last = None\n    for _ in range(10):\n        await page.wait_for_load_state(\"networkidle\")\n        new = page.url\n        if new == last:\n            break\n        last = new\n    try:\n        await page.wait_for_selector(\"table\",state=\"attached\")\n    except Exception as e:\n        print(e)\n    element = await page.query_selector(\"table\")\n    html = await element.inner_html()\n    soup = BeautifulSoup(html, \"html.parser\")\n    await page.close()\n    return soup\n\nasync def main(link):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True,args=[\"--disable-gpu\", \"--no-sandbox\"])\n        context = await browser.new_context()\n        result = await fetchz(link,context)\n        await browser.close()\n        return result\ndates={}\nimport nest_asyncio\nnest_asyncio.apply()\nfrom datetime import date\ntoday=date.today()\ntoday=str(today)\ntoday=today.replace(\"-\",\"\")\ndates=[float(today)]\narrw=[]\nfor d in dates:\n    d=str(d)\n    d=d.replace(\".0\",\"\")\n    url=\"https://barttorvik.com/trank.php?year=2026&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20251103&end=\"+str(d)+\"&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\"\n    print(url)\n    loop = asyncio.get_event_loop()\n    table = loop.run_until_complete(main(url))\n    for tr in table.find_all(\"tr\"):\n        if \"AdjOE\" not in str(tr) and 'absolute error' not in str(tr):\n            rowz=[]\n            n=0\n            for td in tr.find_all(\"td\"):\n                if n!=4:\n                    rowz.append(td.text)\n                else:\n                    fd=td.find_all(\"a\")[0].text\n                    fd=str(fd)\n                    if fd=='':\n                        print(tr.find_all(\"td\"))\n                    rowz.append(fd)\n                n+=1\n            rowz.append(2025)\n            arrw.append(rowz)\ndf=pd.DataFrame(arrw)\ndf.columns=['RK','team','conf','G','Rec','ADJOE','ADJDE','BARTHAG','EFG','EFGD','TOR','TORD','ORB','DRB','FTR','FTRD','2P','2PD','3P','3PD','3PR','3PRD','ADJT','WAB','season']\ndf=df[df.RK!=2026]\narrw=[]\nfor d in dates:\n    d=str(d)\n    d=d.replace(\".0\",\"\")\n    url=\"https://barttorvik.com/net4cast.php\"\n    print(url)\n    loop = asyncio.get_event_loop()\n    table = loop.run_until_complete(main(url))\n    for tr in table.find_all(\"tr\"):\n        if \"AdjOE\" not in str(tr) and 'absolute error' not in str(tr) and 'QUAD' not in str(tr):\n            rowz=[]\n            n=0\n            for td in tr.find_all(\"td\"):\n                if n!=2:\n                    rowz.append(td.text)\n                else:\n                    fd=td.find_all(\"a\")[0].text\n                    fd=str(fd)\n                    if fd=='':\n                        print(tr.find_all(\"td\"))\n                    rowz.append(fd)\n                n+=1\n            rowz.append(2025)\n            arrw.append(rowz)\nprint('done scraping')\ndfa=pd.DataFrame(arrw)\ndfa.columns=['net','NOW','team','QUAD1','QUAD2','QUAD3','QUAD4','season']\ndfa=dfa[dfa.net!=2026]\ndf=df[df.RK!=2025]\ndf=df[df.RK!=2026]\ndfa=dfa[dfa.net!=2025]\nconfs={}\nfor row in df.itertuples():\n    s = row.team\n    result = s.split(\"\\xa0\\xa0\\xa0\", 1)[0]\n    df.at[row.Index,\"team\"]=result\n\nfor row in df.itertuples():\n    confs[row.team]=row.conf\ndfa['conf']='?'\nfor row in dfa.itertuples():\n    s = row.team\n    result = s.split(\"\\xa0\\xa0\\xa0\", 1)[0]\n    dfa.at[row.Index,\"team\"]=result\nfor row in dfa.itertuples():\n    dfa.at[row.Index,'conf']=confs[row.team]\ndf=dfa\ndf=df.astype({\"QUAD1\":\"string\",\"QUAD2\":\"string\",\"QUAD3\":\"string\",\"QUAD4\":\"string\"})\nfor row in df.itertuples():\n    df.at[row.Index,'q1w']=float(str(row.QUAD1).split(\"-\")[0])\n    df.at[row.Index,'q1l']=float(str(row.QUAD1).split(\"-\")[1])\n    df.at[row.Index,'q2w']=float(str(row.QUAD2).split(\"-\")[0])\n    df.at[row.Index,'q2l']=float(str(row.QUAD2).split(\"-\")[1])\n    df.at[row.Index,'q3w']=float(str(row.QUAD3).split(\"-\")[0])\n    df.at[row.Index,'q3l']=float(str(row.QUAD3).split(\"-\")[1])\n    df.at[row.Index,'q4w']=float(str(row.QUAD4).split(\"-\")[0])\n    df.at[row.Index,'q4l']=float(str(row.QUAD4).split(\"-\")[1])\ndf.at[row.Index,'Rec']=''\nfor row in df.itertuples():\n    w=float(row.q1w)+float(row.q2w)+float(row.q3w)+float(row.q4w)\n    l=float(row.q1l)+float(row.q2l)+float(row.q3l)+float(row.q4l)\n    df.at[row.Index,'Rec']=str(w)+\"-\"+str(l)\ndef safe_apply_class_weight(model):\n    \"\"\"\n    Safely applies class_weight='balanced' to any sklearn classifier,\n    including meta-estimators. Never raises errors.\n    \"\"\"\n    # Case 1: model supports class_weight directly\n    try:\n        model.set_params(class_weight=\"balanced\")\n        return model\n    except Exception:\n        pass\n\n    # Case 2: model has base_estimator (CalibratedClassifierCV, etc.)\n    if hasattr(model, \"base_estimator\") and model.base_estimator is not None:\n        try:\n            model.base_estimator.set_params(class_weight=\"balanced\")\n            return model\n        except Exception:\n            pass\n\n    # Case 3: model has estimator (OneVsRest, OneVsOne, MultiOutput)\n    if hasattr(model, \"estimator\") and model.estimator is not None:\n        try:\n            model.estimator.set_params(class_weight=\"balanced\")\n            return model\n        except Exception:\n            pass\n\n    # Case 4: model has estimators_ (Voting, Stacking after fit)\n    if hasattr(model, \"estimators_\"):\n        for est in model.estimators_:\n            try:\n                est.set_params(class_weight=\"balanced\")\n            except Exception:\n                pass\n        return model\n\n    # Case 5: model has estimators (Voting, Stacking before fit)\n    if hasattr(model, \"estimators\"):\n        for name, est in model.estimators:\n            if est is not None:\n                try:\n                    est.set_params(class_weight=\"balanced\")\n                except Exception:\n                    pass\n        return model\n\n    # If nothing worked, return unchanged\n    return model\ndf['team']=df['team'].dropna()\n\nx=[]\ny=[]\nimport os\nu=0\narrp=[]\nimport requests\nfrom bs4 import BeautifulSoup\nimport numpy as np\ns=2024\ncs={}\nu=0\npct={}\nvotes=[]\nvotes=[]\npcts={}\nyw=0\nfor i in range(0,12):\n    pcts[str(i)]={}\nbartdf=pd.read_csv(\"https://cfbzzz.alwaysdata.net/bracketology/goodbart.txt\")\nbartdf=bartdf.drop(\"WAB\",axis=1)\naldf=pd.read_csv(\"https://cfbzzz.alwaysdata.net/bracketology/al.txt\")\nfor i in range(2013,2026):\n    print(i)\n    bartdf=bartdf[bartdf.RK!=i]\nbartdf=bartdf.replace('\\xa0', ' ', regex=True)\nbartdf['ty']=False\nfor row in bartdf.itertuples():\n    if \"   \" in str(row.team):\n        bartdf.at[row.Index,'ty']=True\n        teamh=str(row.team).split(\"   \")[0]\n        bartdf.at[row.Index,'team']=teamh\nbartdf['team']=bartdf['team'].str.replace(\" St.\",\" State\")\nbartdf=bartdf.replace(\"Albany\",\"Albany (NY)\")\nbartdf=bartdf.replace(\"BYU\",\"Brigham Young\")\nbartdf=bartdf.replace(\"Grambling State\",\"Grambling\")\nbartdf=bartdf.replace(\"VCU\",\"Virginia Commonwealth\")\nbartdf=bartdf.replace(\"Fairleigh Dickinson\",\"FDU\")\nbartdf=bartdf.replace(\"SIU Edwardsville\",\"Southern Illinois-Edwardsville\")\nbartdf=bartdf.replace(\"LIU\",\"Long Island University\")\nbartdf=bartdf.replace(\"Nebraska Omaha\",\"Omaha\")\nbartdf=bartdf.replace(\"UMBC\",\"Maryland-Baltimore County\")\nbartdf=bartdf.replace(\"Miami FL\",\"Miami (FL)\")\nbartdf=bartdf.replace(\"SMU\",\"Southern Methodist\")\nbartdf=bartdf.replace(\"Penn\",\"Pennsylvania\")\nbartdf=bartdf.replace(\"Arkansas Pine Bluff\",\"Arkansas-Pine Bluff\")\nbartdf=bartdf.replace(\"USC Upstate\",\"South Carolina Upstate\")\nbartdf=bartdf.replace(\"St. Francis NY\",\"St. Francis (NY)\")\nbartdf=bartdf.replace(\"UMKC\",\"Kansas City\")\nbartdf=bartdf.replace(\"Central Connecticut\",\"Central Connecticut State\")\nbartdf=bartdf.replace(\"Tennessee Martin\",\"Tennessee-Martin\")\nbartdf=bartdf.replace(\"Saint Francis\",\"Saint Francis (PA)\")\nbartdf=bartdf.replace(\"Illinois Chicago\",\"Illinois-Chicago\")\nbartdf=bartdf.replace(\"Bethune Cookman\",\"Bethune-Cookman\")\nbartdf=bartdf.replace(\"N.C. State\",\"NC State\")\nbartdf=bartdf.replace(\"Charleston\",\"College of Charleston\")\nbartdf=bartdf.replace(\"Maryland Eastern Shore\",\"Maryland-Eastern Shore\")\nbartdf=bartdf.replace(\"Loyola Chicago\",\"Loyola (IL)\")\nbartdf=bartdf.replace(\"Louisiana Monroe\",\"Louisiana-Monroe\")\nbartdf=bartdf.replace(\"Southern Miss\",\"Southern Mississippi\")\nbartdf=bartdf.replace(\"Texas A&M Corpus Chris\",\"Texas A&M-Corpus Christi\")\nbartdf=bartdf.replace(\"Saint Mary's\",\"Saint Mary's (CA)\")\nbartdf=bartdf.replace(\"UT Rio Grande Valley\",\"Texas-Rio Grande Valley\")\nbartdf=bartdf.replace(\"Gardner Webb\",\"Gardner-Webb\")\nbartdf=bartdf.replace(\"LSU\",\"Louisiana State\")\nbartdf=bartdf.replace(\"Loyola MD\",\"Loyola (MD)\")\nbartdf=bartdf.replace(\"UMass Lowell\",\"Massachusetts-Lowell\")\nbartdf=bartdf.replace(\"UNLV\",\"Nevada-Las Vegas\")\nbartdf=bartdf.replace(\"USC\",\"Southern California\")\nbartdf=bartdf.replace(\"Sam Houston State\",\"Sam Houston\")\nbartdf=bartdf.replace(\"Prairie View A&M\",\"Prairie View\")\nbartdf=bartdf.replace(\"Miami OH\",\"Miami (OH)\")\nbartdf=bartdf.replace(\"VMI\",\"Virginia Military Institute\")\nbartdf=bartdf.replace(\"St. John's\",\"St. John's (NY)\")\nbartdf=bartdf.replace(\"FIU\",\"Florida International\")\nbartdf=bartdf.replace(\"Queens\",\"Queens (NC)\")\nbartdf=bartdf.replace(\"Cal Baptist\",\"California Baptist\")\nbartdf=bartdf.replace(\"Mount State Mary's\",\"Mount St. Mary's\")\nalargez={}\nfor i in range(1949,2026):\n    alargez[float(i)]=[]\nfor row in aldf.itertuples():\n    if float(row.TY)==1:\n        alargez[float(row.Year)].append(row.Team)\nfor row in bartdf.itertuples():\n    if row.team in alargez[float(row.season)]:\n        bartdf.at[row.Index,'alarge']=1\n    elif row.ty==True:\n        bartdf=bartdf.drop(row.Index)\n    else:\n        bartdf.at[row.Index,'alarge']=0\nconft={}\no=0\nfor cf in set(bartdf.conf.tolist()):\n    conft[cf]=0\n    o+=1  \nbartdf['pct']=0\nbartdf=bartdf[bartdf.season>2017]\nfor row in bartdf.itertuples():\n    bartdf.at[row.Index,'net']=rez[int(row.season)][row.team]['net']\n    bartdf.at[row.Index,'q1w']=rez[int(row.season)][row.team]['q1w']\n    bartdf.at[row.Index,'q2w']=rez[int(row.season)][row.team]['q2w']\n    bartdf.at[row.Index,'q3w']=rez[int(row.season)][row.team]['q3w']\n    bartdf.at[row.Index,'q4w']=rez[int(row.season)][row.team]['q4w']\n    bartdf.at[row.Index,'q1l']=rez[int(row.season)][row.team]['q1l']\n    bartdf.at[row.Index,'q2l']=rez[int(row.season)][row.team]['q2l']\n    bartdf.at[row.Index,'q3l']=rez[int(row.season)][row.team]['q3l']\n    bartdf.at[row.Index,'q4l']=rez[int(row.season)][row.team]['q4l']\nfor row in bartdf.itertuples():\n    rec=str(row.Rec)\n    w=str(rec).split(\"–\")[0]\n    l=str(rec).split(\"–\")[1]\n    w=float(w)\n    l=float(l)\n    pctz=1/(w+l)*w\n    pct=str(row.Rec)\n    w=pct.split(\"–\")[0]\n    l=pct.split(\"–\")[1]\n    w=float(w)\n    l=float(l)\n    pct=w/(w+l)\n    bartdf.at[row.Index,'pct']=pct\ndef add_quad_sos_features(dfz):\n\n    # -----------------------------\n    # Total games\n    # -----------------------------\n    dfz[\"games\"] = (\n        dfz[\"q1w\"] + dfz[\"q1l\"] +\n        dfz[\"q2w\"] + dfz[\"q2l\"] +\n        dfz[\"q3w\"] + dfz[\"q3l\"] +\n        dfz[\"q4w\"] + dfz[\"q4l\"]\n    )\n    \n    # -----------------------------\n    # Quadrant game counts\n    # -----------------------------\n    dfz[\"q1_games\"] = dfz[\"q1w\"] + dfz[\"q1l\"]\n    dfz[\"q2_games\"] = dfz[\"q2w\"] + dfz[\"q2l\"]\n    dfz[\"q3_games\"] = dfz[\"q3w\"] + dfz[\"q3l\"]\n    dfz[\"q4_games\"] = dfz[\"q4w\"] + dfz[\"q4l\"]\n    dfz['q4_dominance']=dfz['q4_games'] / dfz['games']\n    dfz['q4_loss_ratio']=dfz['q4l'] / dfz['q4_games']\n    # Q1A games (premium Q1)\n\n    # -----------------------------\n    # Quadrant shares\n    # -----------------------------\n    safe_games = dfz[\"games\"].replace(0, 1)\n\n    dfz[\"q1_share\"] = dfz[\"q1_games\"] / safe_games\n    dfz[\"q2_share\"] = dfz[\"q2_games\"] / safe_games\n    dfz[\"q3_share\"] = dfz[\"q3_games\"] / safe_games\n    dfz[\"q4_share\"] = dfz[\"q4_games\"] / safe_games\n\n\n\n    # -----------------------------\n    # Opportunity / cupcake indicators\n    # -----------------------------\n    dfz[\"hi_opportunity_share\"] = (dfz[\"q1_games\"] + dfz[\"q2_games\"]) / safe_games\n    dfz[\"cupcake_share\"] = (dfz[\"q3_games\"] + dfz[\"q4_games\"]) / safe_games\n\n    # -----------------------------\n    # Weighted SOS proxy\n    # -----------------------------\n    dfz[\"quad_sos\"] = (\n        4 * dfz[\"q1_games\"] +\n        3 * dfz[\"q2_games\"] +\n        2 * dfz[\"q3_games\"] +\n        1 * dfz[\"q4_games\"]\n    )\n\n    dfz[\"quad_sos_per_game\"] = dfz[\"quad_sos\"] / safe_games\n\n    # -----------------------------\n    # Weighted win quality\n    # (Q1A wins > Q1 wins > Q2 wins > Q3 wins > Q4 wins)\n    # -----------------------------\n    dfz[\"quad_win_quality\"] = (                             # premium wins\n        4 * dfz[\"q1w\"] +                  # other Q1 wins\n        3 * dfz[\"q2w\"] +\n        2 * dfz[\"q3w\"] +\n        1 * dfz[\"q4w\"]\n    )\n\n    # -----------------------------\n    # Weighted loss severity\n    # (Q4 losses worst, Q1A losses least harmful)\n    # -----------------------------\n    dfz[\"quad_loss_severity\"] = (\n        5 * dfz[\"q4l\"] +                                 # worst losses\n        4 * dfz[\"q3l\"] +\n        2 * dfz[\"q2l\"] +\n        1 * dfz[\"q1l\"]                              # Q1A losses least harmful\n    )\n\n    return dfz\n\nbartdf=add_quad_sos_features(bartdf)\nm=0\nbartdf=bartdf[bartdf.season>2017]\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# Add these imports at the top of your file\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neural_network import MLPClassifier\nbartdf=bartdf.fillna(0)\nnumeric_features = ['pct','net',\"q1w\",\"q2w\",\"q3w\",\"q4w\",'q1l','q2l','q3l','q4l','q1_games','q2_games',\n                   'q3_games','q4_games','hi_opportunity_share','cupcake_share','quad_sos_per_game','quad_win_quality','quad_sos','quad_loss_severity','q4_dominance','q4_loss_ratio']\ncategorical_features = ['conf']  # categorical feature(s)\n\n\nx = bartdf[numeric_features + categorical_features]\ny = bartdf['alarge']  # 1 if team made tournament w/o auto bid, 0 otherwise\ny=y.astype(int)\npreprocessor_scaled = ColumnTransformer([\n    ('num', StandardScaler(), numeric_features),\n    ('cat', OneHotEncoder(), categorical_features)\n])\n\n# For models that do not need scaling (trees, ensembles)\npreprocessor_unscaled = ColumnTransformer([\n    ('num', 'passthrough', numeric_features),\n    ('cat', OneHotEncoder(), categorical_features)\n])\npipelines = {}\nfrom sklearn.utils import all_estimators\n\nclassifiers = all_estimators(type_filter=\"classifier\")\nfitted_models = {}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils import all_estimators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Classifiers that need extra parameters and their defaults\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom sklearn.base import BaseEstimator\n\n# ---------------------------\n# 1. Define features\nnumeric_features = ['pct','net',\"q1w\",\"q2w\",\"q3w\",\"q4w\",'q1l','q2l','q3l','q4l','q1_games','q2_games',\n                   'q3_games','q4_games','hi_opportunity_share','cupcake_share','quad_sos_per_game','quad_win_quality','quad_sos','quad_loss_severity','q4_dominance','q4_loss_ratio']\ncategorical_features = ['conf']  # conference\n\n# Assume x is your DataFrame of features, y is your target\n# ---------------------------\n# 2. Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42\n)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils import all_estimators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# Linear models\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier\n\n# Neural networks\nfrom sklearn.neural_network import MLPClassifier\n\n# Tree-based ensembles\nfrom sklearn.ensemble import (ExtraTreesClassifier, RandomForestClassifier, \n                              GradientBoostingClassifier, AdaBoostClassifier,\n                              BaggingClassifier, HistGradientBoostingClassifier,\n                              StackingClassifier, VotingClassifier)\n\n# Decision trees\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\n# Naive Bayes\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB, GaussianNB, CategoricalNB\n\n# SVM / linear models\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.linear_model import SGDClassifier, RidgeClassifier, RidgeClassifierCV, Perceptron\n\n# Meta-classifiers\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Discriminant analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\n# Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n# ---------------------------\n# 3. List all classifiers\nclassifiers = all_estimators(type_filter=\"classifier\")\nfitted_models = {}\n\n# ---------------------------\n# 4. Loop through classifiers\nfrom sklearn.utils import all_estimators\n\ndef supports_class_weight(Clf):\n    \"\"\"\n    Returns True if the classifier supports class_weight.\n    \"\"\"\n    try:\n        # Try to instantiate with no args\n        model = Clf()\n    except TypeError:\n        # If the model requires arguments, inspect its signature instead\n        try:\n            params = Clf().get_params()\n        except:\n            return False\n    else:\n        params = model.get_params()\n\n    return \"class_weight\" in params\n\n\n# Example: test all classifiers\nclassifiers = all_estimators(type_filter=\"classifier\")\ndef safe_apply_class_weight(model):\n    \"\"\"\n    Safely applies class_weight='balanced' to any sklearn classifier,\n    including meta-estimators. Never raises errors.\n    \"\"\"\n    # Case 1: model supports class_weight directly\n    try:\n        model.set_params(class_weight=\"balanced\")\n        return model\n    except Exception:\n        pass\n\n    # Case 2: model has base_estimator (CalibratedClassifierCV, etc.)\n    if hasattr(model, \"base_estimator\") and model.base_estimator is not None:\n        try:\n            model.base_estimator.set_params(class_weight=\"balanced\")\n            return model\n        except Exception:\n            pass\n\n    # Case 3: model has estimator (OneVsRest, OneVsOne, MultiOutput)\n    if hasattr(model, \"estimator\") and model.estimator is not None:\n        try:\n            model.estimator.set_params(class_weight=\"balanced\")\n            return model\n        except Exception:\n            pass\n\n    # Case 4: model has estimators_ (Voting, Stacking after fit)\n    if hasattr(model, \"estimators_\"):\n        for est in model.estimators_:\n            try:\n                est.set_params(class_weight=\"balanced\")\n            except Exception:\n                pass\n        return model\n\n    # Case 5: model has estimators (Voting, Stacking before fit)\n    if hasattr(model, \"estimators\"):\n        for name, est in model.estimators:\n            if est is not None:\n                try:\n                    est.set_params(class_weight=\"balanced\")\n                except Exception:\n                    pass\n        return model\n\n    # If nothing worked, return unchanged\n    return model\n\n\nfrom sklearn.utils import all_estimators\n\ndef supports_predict_proba(Clf):\n    \"\"\"\n    Returns True if a classifier supports predict_proba.\n    Handles meta-estimators and models requiring special parameters.\n    \"\"\"\n    try:\n        # Try to instantiate with default params\n        model = Clf()\n    except Exception:\n        # If instantiation fails, skip this model\n        return False\n\n    # Case 1: model has predict_proba directly\n    if hasattr(model, \"predict_proba\"):\n        return True\n\n    # Case 2: meta-estimators with base estimator\n    if hasattr(model, \"base_estimator\") and model.base_estimator is not None:\n        if hasattr(model.base_estimator, \"predict_proba\"):\n            return True\n\n    # Case 3: meta-estimators with estimator attribute\n    if hasattr(model, \"estimator\") and model.estimator is not None:\n        if hasattr(model.estimator, \"predict_proba\"):\n            return True\n\n    return False\n\n\ndef filter_models_with_predict_proba(model_names):\n    \"\"\"\n    Given a list of model names (strings), return only those that support predict_proba.\n    \"\"\"\n    classifiers = dict(all_estimators(type_filter=\"classifier\"))\n    supported = []\n\n    for name in model_names:\n        if name in classifiers:\n            Clf = classifiers[name]\n            if supports_predict_proba(Clf):\n                supported.append(name)\n\n    return supported\n\nnames=filter_models_with_predict_proba(acc)\nnames=names[:12]\nnames=['LogisticRegressionCV', 'LogisticRegression', 'CalibratedClassifierCV_LinearSVC', 'CalibratedClassifierCV_RidgeClassifier', 'CalibratedGradientBoostingClassifier', 'CalibratedHistGradientBoostingClassifier', 'CalibratedRandomForestClassifier', 'CalibratedExtraTreesClassifier', 'CalibratedBaggingClassifier', 'CalibratedGradientBoostingClassifier_v2', 'CalibratedHistGradientBoostingClassifier_v2', 'CalibratedRandomForestClassifier_v2']\nfitted_models={}\nprint(\"fitting models for prediction...\")\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    HistGradientBoostingClassifier,\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    BaggingClassifier\n)\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\ndf=df.replace('\\xa0', ' ', regex=True)\nfor row in df.itertuples():\n    if \"   \" in str(row.team):\n        teamh=str(row.team).split(\"   \")[0]\n        df.at[row.Index,'team']=teamh\ndf['team']=df['team'].str.replace(\" St.\",\" State\")\ndf=df.replace(\"Albany\",\"Albany (NY)\")\ndf=df.replace(\"BYU\",\"Brigham Young\")\ndf=df.replace(\"Grambling State\",\"Grambling\")\ndf=df.replace(\"VCU\",\"Virginia Commonwealth\")\ndf=df.replace(\"Fairleigh Dickinson\",\"FDU\")\ndf=df.replace(\"SIU Edwardsville\",\"Southern Illinois-Edwardsville\")\ndf=df.replace(\"LIU\",\"Long Island University\")\ndf=df.replace(\"Nebraska Omaha\",\"Omaha\")\ndf=df.replace(\"UMBC\",\"Maryland-Baltimore County\")\ndf=df.replace(\"Miami FL\",\"Miami (FL)\")\ndf=df.replace(\"SMU\",\"Southern Methodist\")\ndf=df.replace(\"Penn\",\"Pennsylvania\")\ndf=df.replace(\"Arkansas Pine Bluff\",\"Arkansas-Pine Bluff\")\ndf=df.replace(\"USC Upstate\",\"South Carolina Upstate\")\ndf=df.replace(\"St. Francis NY\",\"St. Francis (NY)\")\ndf=df.replace(\"UMKC\",\"Kansas City\")\ndf=df.replace(\"Central Connecticut\",\"Central Connecticut State\")\ndf=df.replace(\"Tennessee Martin\",\"Tennessee-Martin\")\ndf=df.replace(\"Saint Francis\",\"Saint Francis (PA)\")\ndf=df.replace(\"Illinois Chicago\",\"Illinois-Chicago\")\ndf=df.replace(\"Bethune Cookman\",\"Bethune-Cookman\")\ndf=df.replace(\"N.C. State\",\"NC State\")\ndf=df.replace(\"Charleston\",\"College of Charleston\")\ndf=df.replace(\"Maryland Eastern Shore\",\"Maryland-Eastern Shore\")\ndf=df.replace(\"Loyola Chicago\",\"Loyola (IL)\")\ndf=df.replace(\"Louisiana Monroe\",\"Louisiana-Monroe\")\ndf=df.replace(\"Southern Miss\",\"Southern Mississippi\")\ndf=df.replace(\"Texas A&M Corpus Chris\",\"Texas A&M-Corpus Christi\")\ndf=df.replace(\"Saint Mary's\",\"Saint Mary's (CA)\")\ndf=df.replace(\"UT Rio Grande Valley\",\"Texas-Rio Grande Valley\")\ndf=df.replace(\"Gardner Webb\",\"Gardner-Webb\")\ndf=df.replace(\"LSU\",\"Louisiana State\")\ndf=df.replace(\"Loyola MD\",\"Loyola (MD)\")\ndf=df.replace(\"UMass Lowell\",\"Massachusetts-Lowell\")\ndf=df.replace(\"UNLV\",\"Nevada-Las Vegas\")\ndf=df.replace(\"USC\",\"Southern California\")\ndf=df.replace(\"Sam Houston State\",\"Sam Houston\")\ndf=df.replace(\"Prairie View A&M\",\"Prairie View\")\ndf=df.replace(\"Miami OH\",\"Miami (OH)\")\ndf=df.replace(\"VMI\",\"Virginia Military Institute\")\ndf=df.replace(\"St. John's\",\"St. John's (NY)\")\ndf=df.replace(\"FIU\",\"Florida International\")\ndf=df.replace(\"Queens\",\"Queens (NC)\")\ndf=df.replace(\"Cal Baptist\",\"California Baptist\")\ndf=df.replace(\"Mount State Mary's\",\"Mount St. Mary's\")\nfor row in df.itertuples():\n    df.at[row.Index,'net']=rez[int(row.season)][row.team]['net']\n    df.at[row.Index,'q1w']=rez[int(row.season)][row.team]['q1w']\n    df.at[row.Index,'q2w']=rez[int(row.season)][row.team]['q2w']\n    df.at[row.Index,'q3w']=rez[int(row.season)][row.team]['q3w']\n    df.at[row.Index,'q4w']=rez[int(row.season)][row.team]['q4w']\n    df.at[row.Index,'q1l']=rez[int(row.season)][row.team]['q1l']\n    df.at[row.Index,'q2l']=rez[int(row.season)][row.team]['q2l']\n    df.at[row.Index,'q3l']=rez[int(row.season)][row.team]['q3l']\n    df.at[row.Index,'q4l']=rez[int(row.season)][row.team]['q4l']\n\nimport pandas as pd\n\ndf=add_quad_sos_features(df)\ndf=df.fillna(0)\nprint(\"prediction time!\")\n# ---------------------------------------------------------\n# 1. Parse Rec column (vectorized)\n# ---------------------------------------------------------\nfor row in df.itertuples():\n    df.at[row.Index,'w']=float(str(row.Rec).split(\"-\")[0])\n    df.at[row.Index,'l']=float(str(row.Rec).split(\"-\")[1])\ndf['pct'] = df['w'] / (df['w'] + df['l'])\n\n# ---------------------------------------------------------\n# 2. Build the feature matrix ONCE\n# ---------------------------------------------------------\n\nfeature_cols = [\n    'conf','pct','net','q1w','q2w','q3w','q4w','q1l','q2l','q3l','q4l',\n    'q1_games','q2_games','q3_games','q4_games',\n    'hi_opportunity_share','cupcake_share','quad_sos_per_game','quad_win_quality','quad_sos',\n    'quad_loss_severity','q4_dominance','q4_loss_ratio'\n]\n\nX = df[feature_cols]\nteams = df['team'].tolist()\n\n# ---------------------------------------------------------\n# 3. Run each model ONCE on the entire dataset\n# ---------------------------------------------------------\n\nimport numpy as np\nimport pandas as pd\ndf_current=df\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    HistGradientBoostingClassifier,\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    BaggingClassifier,\n    StackingClassifier,\n)\n\n# =========================================\n# 0. YOUR DATA\n# =========================================\n\n# bartdf: past seasons, with target 'alarge'\n# df_current: current season teams\n# numeric_features, categorical_features: same as before\n\nX_all = bartdf[numeric_features + categorical_features]\ny_all = bartdf['alarge'].astype(int)\n\nX_current = df_current[numeric_features + categorical_features]\n\n# Column transformers (define ONCE and reuse)\npreprocessor_linear = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\n\npreprocessor_tree = ColumnTransformer(\n    transformers=[\n        (\"num\", \"passthrough\", numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\n\n# =========================================\n# 1. Define the 12 base estimators as pipelines\n# =========================================\n\n\n\nbase_estimators = []\n\n# -- 3 linear / margin models\nbase_estimators.append(\n    (\"LogisticRegressionCV\",\n     Pipeline([\n         (\"pre\", preprocessor_linear),\n         (\"clf\", LogisticRegressionCV(\n             cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n             solver=\"liblinear\",\n             max_iter=5000\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"LogisticRegression\",\n     Pipeline([\n         (\"pre\", preprocessor_linear),\n         (\"clf\", LogisticRegression(\n             C=1.0,\n             class_weight=\"balanced\",\n             max_iter=5000,\n             random_state=42\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_LinearSVC\",\n     Pipeline([\n         (\"pre\", preprocessor_linear),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=LinearSVC(random_state=42),\n             cv=5\n         ))\n     ]))\n)\n\n# -- 4 boosting models\nbase_estimators.append(\n    (\"Calibrated_GBC\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=GradientBoostingClassifier(random_state=42),\n             cv=5\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_GBC_v2\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=GradientBoostingClassifier(\n                 random_state=42,\n                 max_depth=3,\n                 learning_rate=0.05\n             ),\n             cv=5\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_HGBC\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=HistGradientBoostingClassifier(random_state=42),\n             cv=5\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_HGBC_v2\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=HistGradientBoostingClassifier(\n                 random_state=42,\n                 max_depth=3,\n                 learning_rate=0.05\n             ),\n             cv=5\n         ))\n     ]))\n)\n\n# -- 3 RF / ET\nbase_estimators.append(\n    (\"Calibrated_RF\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=RandomForestClassifier(\n                 n_estimators=500,\n                 max_depth=10,\n                 min_samples_leaf=5,\n                 class_weight=\"balanced\",\n                 random_state=42\n             ),\n             cv=5\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_RF_v2\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=RandomForestClassifier(\n                 n_estimators=300,\n                 max_depth=6,\n                 min_samples_leaf=3,\n                 class_weight=\"balanced\",\n                 random_state=42\n             ),\n             cv=5\n         ))\n     ]))\n)\n\nbase_estimators.append(\n    (\"Calibrated_ET\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=ExtraTreesClassifier(\n                 n_estimators=500,\n                 max_depth=10,\n                 min_samples_leaf=5,\n                 class_weight=\"balanced\",\n                 random_state=42\n             ),\n             cv=5\n         ))\n     ]))\n)\n\n# -- 1 bagging\nbase_estimators.append(\n    (\"Calibrated_Bagging\",\n     Pipeline([\n         (\"pre\", preprocessor_tree),\n         (\"clf\", CalibratedClassifierCV(\n             base_estimator=BaggingClassifier(\n                 n_estimators=200,\n                 random_state=42\n             ),\n             cv=5\n         ))\n     ]))\n)\n\n# =========================================\n# 2. Stacking classifier (fast meta logistic regression)\n# =========================================\nprint(\"appended estimators\")\n# final_estimator is your logistic stacker on base predict_proba outputs\nfinal_estimator = LogisticRegression(\n    C=1.0,\n    penalty=\"l2\",\n    solver=\"liblinear\",\n    max_iter=5000,\n    random_state=42\n)\nprint('done final estimator')\nstack = StackingClassifier(\n    estimators=base_estimators,\n    final_estimator=final_estimator,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n    stack_method=\"predict_proba\",    # use base models' probabilities as features\n    n_jobs=-1                        # parallelize across models where possible\n)\nprint('done intializing stack')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.isotonic import IsotonicRegression\nimport numpy as np\nimport pandas as pd\n\nprint(\"Fast calibration using single train/test split...\")\nX_hist=x\ny_hist=y\n# 1. SINGLE SPLIT instead of 5-fold CV (80/20 is plenty for calibration)\nX_tr, X_calib, y_tr, y_calib = train_test_split(\n    X_hist, y_hist, test_size=0.2, stratify=y_hist, random_state=42\n)\n\n# 2. Fit stack ONCE on training split\n\nstack.fit(X_tr, y_tr)\nprint(\"done stack\")\n# 3. Get OOF probabilities ONCE on calibration split\nhist_probs = stack.predict_proba(X_calib)[:, 1]\nhist_labels = y_calib.values if hasattr(y_calib, 'values') else np.asarray(y_calib)\n\n# 4. Fit isotonic (very fast)\niso_calibrator = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\niso_calibrator.fit(hist_probs, hist_labels)\nprint(\"done iso fit\")\nprint(\"done stack 2\")\n# 5. Fit FINAL stack on ALL historical data (one more fit)\nstack.fit(X_hist, y_hist)\n\n# 6. Predict current season and calibrate (very fast)\nraw_probs_current = stack.predict_proba(X_current)[:, 1]\ncalibrated_probs_current = iso_calibrator.predict(raw_probs_current)\n\n# 7. Tail adjustment (vectorized, instant)\nalpha = 0.5          # strength of tail adjustment (0 = none, 1 = full isotonic)\ntail_threshold = 0.7 # start adjusting above this raw probability\n\nfinal_probs = raw_probs_current.copy()\ntail_mask = raw_probs_current > tail_threshold\nfinal_probs[tail_mask] = (\n    (1.0 - alpha) * raw_probs_current[tail_mask] +\n    alpha * calibrated_probs_current[tail_mask]\n)\n\n# Attach results - make sure df_current has same length as X_current\ndf=df.reset_index(drop=True)  # Ensure proper indexing\ndf[\"atlarge_prob_raw\"] = raw_probs_current\ndf[\"atlarge_prob_calibrated\"] = calibrated_probs_current\ndf[\"atlarge_prob_final\"] = final_probs\n\nprint(\"Calibration complete!\")\n\n# Fixed Purdue print - safer indexing\npurdue_row = df[df['team'].str.contains('Purdue', case=False, na=False)]\nif len(purdue_row) > 0:\n    print(f\"Purdue final prob: {purdue_row['atlarge_prob_final'].iloc[0]:.1%}\")\nelse:\n    print(\"Purdue not found in df_current\")\n\n\naqs = {}\ndf_current = df_current.astype({\"net\": float})\nfor conf in sorted(set(df_current.conf.tolist())):\n    uy = df_current[(df_current.conf == conf) & (df_current.season == 2025)]\n    if len(uy) == 0:\n        continue\n    uy = uy.sort_values(by=\"net\", ascending=True).head(1)\n    for t in uy.team.tolist():\n        aqs[t] = conf\n\n# Non-AQ candidates, ranked by FINAL calibrated probability\ncandidates = [t for t in df.team.tolist() if t not in aqs]\nteam_final_prob = dict(zip(df.team, df[\"atlarge_prob_final\"]))\n\navg_prob = {t: team_final_prob.get(t, 0.0) for t in candidates}\nsorted_teams = sorted(avg_prob.keys(), key=lambda x: avg_prob[x], reverse=True)\n\n# Take field + FFO + NFO\nN_AT_LARGE = 37\natlarge = sorted_teams[:N_AT_LARGE + 8]\n\n# ========================================================\n# 9. PRINT THE FIELD (same format as your original)\n# ========================================================\nBOLD = '\\033[1m'\nEND = '\\033[0m'\n\nprint(BOLD + \"<h1>IN THE FIELD</h1>\" + END)\nfor t in atlarge[:N_AT_LARGE]:\n    print(f\"<h3>{t}</h3>\")\n\nprint(BOLD + \"<h1>AUTOMATIC QUALIFIERS</h1>\" + END)\nfor t in aqs.keys():\n    print(f\"<h3>{t}{BOLD} (AQ)</h3>{END}\")\n\nprint(BOLD + \"<h1>FIRST FOUR OUT</h1>\" + END)\nfor t in atlarge[N_AT_LARGE:N_AT_LARGE+4]:\n    print(f\"<h3>{t}</h3>\")\n\nprint(BOLD + \"<h1>NEXT FOUR OUT</h1>\" + END)\nfor t in atlarge[N_AT_LARGE+4:N_AT_LARGE+8]:\n    print(f\"<h3>{t}</h3>\")\n\nprint(f\"\\nTotal at-large: {len(atlarge[:N_AT_LARGE])}\")\nprint(f\"Total AQs: {len(aqs)}\")\nprint(f\"Total field: {len(atlarge[:N_AT_LARGE]) + len(aqs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:54:43.033467Z","iopub.execute_input":"2026-01-13T23:54:43.033841Z","iopub.status.idle":"2026-01-13T23:55:28.160643Z","shell.execute_reply.started":"2026-01-13T23:54:43.033810Z","shell.execute_reply":"2026-01-13T23:55:28.158918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df.team=='']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:20:51.705743Z","iopub.execute_input":"2026-01-13T19:20:51.706034Z","iopub.status.idle":"2026-01-13T19:20:51.732332Z","shell.execute_reply.started":"2026-01-13T19:20:51.706015Z","shell.execute_reply":"2026-01-13T19:20:51.731190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mids={}\narrh=[]\nws={}\nls={}\npowz=['SEC','B10','B12','BE','ACC']\nfor row in df.itertuples():\n    if row.conf not in powz:\n        ws[row.team]=row.w\n        ls[row.team]=row.l\n        mids[row.team]=row.atlarge_prob_final\njs=sorted(mids,key=mids.get,reverse=True)[:25]\nfor k in js:\n    num=mids[k]*100\n    if num<1:\n        num=\"<1\"\n    else:\n        num=round(num,1)\n    num=str(num)\n    num=num.replace(\".0\",\"\")\n    num=num+\"%\"\n    arrh.append([k,num,ws[k],ls[k]])\nqa=pd.DataFrame(arrh,columns=['team','prob','Proj W','Proj L'])\nqa.to_html(\"midmjz.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:28:30.863964Z","iopub.execute_input":"2026-01-13T19:28:30.864266Z","iopub.status.idle":"2026-01-13T19:28:30.880705Z","shell.execute_reply.started":"2026-01-13T19:28:30.864221Z","shell.execute_reply":"2026-01-13T19:28:30.879784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nhtml=requests.get(\"https://www.teamrankings.com/ncb/conference-tournaments/\")\nsoup=BeautifulSoup(html.text)\nrats={}\nfor row in df.itertuples():\n    rats[row.team]=row.net\nlinks=soup.find_all(\"a\")\nlinkz={}\nfor link in links:\n    liny=link\n    if 'bracket' in str(link) and 'ncaa' not in str(link) and 'Pac 12' not in str(link):\n        link='https://www.teamrankings.com/ncb/conference-tournaments/'+(link['href'])\n        linkz[str(liny.text).replace(\" Tournament\",\"\").replace(\" Bracket 2026\",\"\")]=link\nneut=['MAC']\nconfg={}\nprobz={}\ndel link[\"Pac 12\"]\nfor link in linkz:\n    print(link)\n    probz[link]={}\n    confg[link]={}\n    url=linkz[link]\n    html=requests.get(url).text\n    soup=BeautifulSoup(html)\n    table=soup.find_all(\"table\")[2]\n    seed=1\n    for tr in table.find_all(\"tr\"):\n        if 'Current' not in str(tr) and 'conf W' not in str(tr):\n            tds=tr.find_all(\"td\")\n            confg[link][seed]=tds[0].text\n            prob=str(tds[8].text).replace(\"%\",\"\")\n            probz[link][tds[0].text]=float(prob)\n            seed+=1\n    if link=='ACC':\n        probzy=sorted(probz[link],key=probz[link].get,reverse=True)[:8]\nprobj=probzy\nprobz={}\nfor k in probj:\n    probz[k]={}\n    s=0\n    for t in probj[k]:\n        probz[k][t]=s\n        s+=1\n    \ndef getw(arro,link):\n    if probz[link][arro[0]]>probz[link][arro[1]]:\n        return probz[link][arro[1]]\n    else:\n        return probz[link][arro[0]]\nfor c in confg:\n    if c=='ACC':\n        print(c)\n        seeds=confg[c]\n        bracket = [seeds[0], seeds[3], seeds[2], seeds[1], seeds[4], seeds[7], seeds[6], seeds[5]]  # Standard 1-8,4-5,2-7,3-6 quarters\n        round1 = [\n            getw([bracket[0],bracket[7]],link),\n            getw([bracket[3],bracket[4]],link),\n            getw([bracket[1],bracket[6]],link),\n            getw([bracket[2],bracket[5]],link)\n        ]\n        round2=[\n            getw([round1[0],round1[1]],link),\n            getw([round1[2],round1[3]],link)\n        ]\n        champ=[\n            getw([round2[0],round2[1]],link)\n        ]\n        print(champ)\n        \n        \n        \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:51:35.643700Z","iopub.execute_input":"2026-01-13T23:51:35.644028Z","iopub.status.idle":"2026-01-13T23:51:36.448444Z","shell.execute_reply.started":"2026-01-13T23:51:35.644002Z","shell.execute_reply":"2026-01-13T23:51:36.446767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def getw(arro,link):\n    if probz[link][arro[0]]>probz[link][arro[1]]:\n        return probz[link][arro[1]]\n    else:\n        return probz[link][arro[0]]\nfor c in confg:\n    if c=='ACC':\n        print(c)\n        seeds=confg[c]\n        bracket = [seeds[0], seeds[3], seeds[2], seeds[1], seeds[4], seeds[7], seeds[6], seeds[5]]  # Standard 1-8,4-5,2-7,3-6 quarters\n        round1 = [\n            getw([bracket[0],bracket[7]],link),\n            getw([bracket[3],bracket[4]],link),\n            getw([bracket[1],bracket[6]],link),\n            getw([bracket[2],bracket[5]],link)\n        ]\n        round2=[\n            getw([round1[0],round1[1]],link),\n            getw([round1[2],round1[3]],link)\n        ]\n        champ=[\n            getw([round2[0],round2[1]],link)\n        ]\n        print(champ)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:41:28.104504Z","iopub.execute_input":"2026-01-13T20:41:28.104836Z","iopub.status.idle":"2026-01-13T20:41:28.121219Z","shell.execute_reply.started":"2026-01-13T20:41:28.104816Z","shell.execute_reply":"2026-01-13T20:41:28.119426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confg[c]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T20:41:43.468429Z","iopub.execute_input":"2026-01-13T20:41:43.469046Z","iopub.status.idle":"2026-01-13T20:41:43.475638Z","shell.execute_reply.started":"2026-01-13T20:41:43.469018Z","shell.execute_reply":"2026-01-13T20:41:43.474646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arrz=[]\narr=[]\nfor t in aqs:\n    stry=t\n    arr.append(stry)\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narry=[]\nfor t in atlarge[:37]:\n    arry.append(t)\narri=[]\nfor t in atlarge[37:41]:\n    arri.append(t)\nfor i in range(0,33):\n    arri.append(\"\")\narrii=[]\nfor t in atlarge[41:45]:\n    arrii.append(t)\nfor i in range(0,33):\n    arrii.append(\"\")\narrw=[]\nfor t in atlarge[33:37]:\n    arrw.append(t)\nfor i in range(0,33):\n    arrw.append(\"\")\nimport numpy as np\narrz = np.stack((arr,arry,arrw,arri,arrii), axis=1)\n\ndfz=pd.DataFrame(arrz,columns=['AQS','At-Large','Last Four In','First Four Out','Next Four Out'])\ndfz.reset_index(drop=True,inplace=True)\ndfz = dfz.style.set_table_styles(\n    [{'selector': 'th', 'props': [('font-size', '20px')]}]\n)\ndfz = dfz.map(lambda x: 'font-size:16px', subset=['AQS','At-Large','Last Four In','First Four Out','Next Four Out'])\ndfz.to_html(\"goodproby.html\",index_col=0)\ndfz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:13:56.173514Z","iopub.execute_input":"2026-01-13T19:13:56.173828Z","iopub.status.idle":"2026-01-13T19:13:56.198748Z","shell.execute_reply.started":"2026-01-13T19:13:56.173808Z","shell.execute_reply":"2026-01-13T19:13:56.197807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"powz=['SEC','B10','B12','BE','ACC']\najk={}\narrq=[]\nfor row in df.itertuples():\n    if 1==1:\n        q=[]\n        if row.conf not in powz:\n            for k in pcts:\n                q.append(pcts[k][row.team])\n            proby=np.mean(q)\n            ajk[row.team]=proby\ntzu=sorted(ajk,key=ajk.get,reverse=True)[:25]\ni=1\nfor k in tzu:\n   arrq.append([i,k,round((ajk[k]*100),1)])\n   i+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:08:07.468747Z","iopub.execute_input":"2026-01-13T19:08:07.469071Z","iopub.status.idle":"2026-01-13T19:08:07.486814Z","shell.execute_reply.started":"2026-01-13T19:08:07.469043Z","shell.execute_reply":"2026-01-13T19:08:07.485534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfs=pd.DataFrame(arrq,columns=[\"rank\",\"team\",\"probability\"])\ndfs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T18:01:57.310884Z","iopub.execute_input":"2026-01-13T18:01:57.311452Z","iopub.status.idle":"2026-01-13T18:01:57.326591Z","shell.execute_reply.started":"2026-01-13T18:01:57.311404Z","shell.execute_reply":"2026-01-13T18:01:57.324696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create a sample DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(4, 2)) # Adjust figure size as needed\n\n# Hide the axis\nax.axis('off')\n\n# Render the DataFrame as a table\ntbl = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\n\n# Optional: Adjust table properties (e.g., scale)\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(10)\ntbl.scale(1.2, 1.2)\n\n# Save the figure as an image file\nplt.savefig(\"dataframe_matplotlib.png\", bbox_inches='tight', dpi=150)\nplt.close() # Close the figure to prevent it from displaying in notebooks\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"op=sorted(ajk,key=ajk.get,reverse=True)[:25]\nfor t in op:\n    probt=ajk[t]\n    probt=probt*100\n    print(t+\": \"+str(probt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:06:43.981945Z","iopub.execute_input":"2026-01-12T01:06:43.982400Z","iopub.status.idle":"2026-01-12T01:06:43.992368Z","shell.execute_reply.started":"2026-01-12T01:06:43.982364Z","shell.execute_reply":"2026-01-12T01:06:43.991270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key in f:\n    if test_accuracy[key]!=f[key]:\n        print(key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T22:43:38.238221Z","iopub.execute_input":"2026-01-02T22:43:38.238599Z","iopub.status.idle":"2026-01-02T22:43:38.242971Z","shell.execute_reply.started":"2026-01-02T22:43:38.238569Z","shell.execute_reply":"2026-01-02T22:43:38.241770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arrz=[]\narr=[]\nfor t in aqs:\n    stry=t+\" (\"+aqs[t]+\")\"\n    arr.append(stry)\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narr.append(\"\")\narry=[]\nfor t in atlarge[:37]:\n    arry.append(t)\narri=[]\nfor t in atlarge[37:41]:\n    arri.append(t)\nfor i in range(0,33):\n    arri.append(\"\")\narrii=[]\nfor t in atlarge[41:45]:\n    arrii.append(t)\nfor i in range(0,33):\n    arrii.append(\"\")\narrw=[]\nfor t in atlarge[33:37]:\n    arrw.append(t)\nfor i in range(0,33):\n    arrw.append(\"\")\nimport numpy as np\narrz = np.stack((arr,arry,arrw,arri,arrii), axis=1)\n\ndfz=pd.DataFrame(arrz,columns=['AQS','At-Large','Last Four In','First Four Out','Next Four Out'])\ndfz = dfz.style.set_table_styles(\n    [{'selector': 'th', 'props': [('font-size', '20px')]}]\n)\ndfz = dfz.map(lambda x: 'font-size:16px', subset=['AQS','At-Large','Last Four In','First Four Out','Next Four Out'])\n\ndfz.to_html(\"tableqawaz.html\")\nprint(\"good\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T02:39:58.762983Z","iopub.execute_input":"2026-01-13T02:39:58.763346Z","iopub.status.idle":"2026-01-13T02:39:58.788782Z","shell.execute_reply.started":"2026-01-13T02:39:58.763318Z","shell.execute_reply":"2026-01-13T02:39:58.787266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teamk={}\nimport numpy as np\nfor tm in set(df.team.tolist()):\n    jk=[]\n    for key in pcts:\n        jk.append(pcts[key][tm])\n    \n    \n    jk=np.mean(jk)\n    teamk[tm]=jk\narrj=[]\nteamkz=sorted(teamk,key=teamk.get,reverse=True)\nfor key in teamkz:\n    pfh=float(teamk[key])\n    pfh=pfh*100\n    pfh=round(pfh,1)\n    arrj.append([key,pfh])\n    print(key,\":\",pfh)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:29:18.237558Z","iopub.execute_input":"2026-01-03T14:29:18.238005Z","iopub.status.idle":"2026-01-03T14:29:18.431134Z","shell.execute_reply.started":"2026-01-03T14:29:18.237969Z","shell.execute_reply":"2026-01-03T14:29:18.430083Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qw=pd.DataFrame(arrj,columns=['team','probability'])\nqw=qw.head(100)\nqw[qw.team=='Miami OH']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T22:38:37.872949Z","iopub.execute_input":"2025-12-31T22:38:37.873434Z","iopub.status.idle":"2025-12-31T22:38:37.887113Z","shell.execute_reply.started":"2025-12-31T22:38:37.873390Z","shell.execute_reply":"2025-12-31T22:38:37.885884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qw.to_html(\"probs.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:36:29.283766Z","iopub.execute_input":"2025-12-30T20:36:29.284102Z","iopub.status.idle":"2025-12-30T20:36:29.295452Z","shell.execute_reply.started":"2025-12-30T20:36:29.284076Z","shell.execute_reply":"2025-12-30T20:36:29.294366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfw=pd.read_csv(\"https://ontheroadtovote.com/alarge.csv\")\ndfw=dfw[dfw.Year>2006]\ndfw=dfw[dfw.TY==1]\ndfw=dfw.replace(\"Louisiana State\",\"LSU\")\ndfw=dfw.replace(\"Loyola (MD)\",\"Loyola MD\")\ndfw=dfw.replace(\"Fairleigh-Dickinson\",\"FDU\")\ndfw=dfw.replace(\"Fairleigh Dickinson\",\"FDU\")\ndfw=dfw.replace(\"Southern California\",\"USC\")\ndfw=dfw.replace(\"Miami (FL)\",\"Miami FL\")\ndfw=dfw.replace(\"Brigham Young\",\"BYU\")\ndfw=dfw.replace(\"St. John's (NY)\",\"St. John's\")\ndfw=dfw.replace(\"Nevada-Las Vegas\",\"UNLV\")\ndfw=dfw.replace(\"Saint Mary's (CA)\",\"Saint Mary's\")\ndfw=dfw.replace(\"Virginia Commonwealth\",\"VCU\")\ndfw=dfw.replace(\"Southern Mississippi\",\"Southern Miss\")\ndfw=dfw.replace(\"NC State\",\"N.C. State\")\nte=set(dfw.Team.tolist())\nsr={}\n\n    \ndf['team']=df['team'].str.replace(\" St.\",\" State\")\nfor row in dfw.itertuples():\n    if float(row.Year) not in sr:\n        sr[float(row.Year)]={}\n    sr[float(row.Year)][str(row.Team)]=float(row.TY)\n    \nfor row in df.itertuples():\n    if row.team in sr[float(row.season)]:\n        df.at[row.Index,'alarge']=sr[float(row.season)][row.team]\n    else:\n        df.at[row.Index,'alarge']=0\ndf.to_csv(\"barts.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T17:13:00.452224Z","iopub.status.idle":"2025-12-23T17:13:00.452546Z","shell.execute_reply":"2025-12-23T17:13:00.452414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from bs4 import BeautifulSoup\narr=[]\nimport requests\n\nfor s in range(2008,2025):\n    s=str(s)\n    url=\"https://barttorvik.com/trank.php?begin=\"+str(seasons[s][0])+\"&end=\"+str(seasons[s][1])+\"&conlimit=All&year=\"+str(s)+\"&top=0&hteam=&quad=5&rpi=\"\n    html=requests.get(url).text\n    print(s)\n    soup=BeautifulSoup(html)\n    table=soup.find_all(\"table\")[0]\n    for tr in table.find_all(\"tr\"):\n        if \"Rk\" not in str(tr):\n            tds=tr.find_all(\"td\")\n            row=[]\n            row.append(float(s)-1)\n            for i in range(0,len(tds)):\n                if i==4:\n                    y=tds[i].find(\"a\").text\n                    x=tds[i].find(\"span\").text\n                    row.append(y)\n                    row.append(x)\n                else:\n                    row.append(tds[i].text)\n            arr.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T17:14:52.852472Z","iopub.execute_input":"2025-12-23T17:14:52.852866Z","iopub.status.idle":"2025-12-23T17:14:53.667213Z","shell.execute_reply.started":"2025-12-23T17:14:52.852836Z","shell.execute_reply":"2025-12-23T17:14:53.665964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install playwright\n!playwright install\n!playwright install-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T23:24:32.513874Z","iopub.execute_input":"2026-01-14T23:24:32.514124Z","iopub.status.idle":"2026-01-14T23:26:18.356486Z","shell.execute_reply.started":"2026-01-14T23:24:32.514102Z","shell.execute_reply":"2026-01-14T23:26:18.354768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seasons={\"2008\":[\"20071105\",\"20080316\"],\"2009\":[\"20081110\",\"20090310\"],\"2010\":[\"20091109\",\"20100309\"],\"2011\":[\"20101108\",\"20110313\"],\"2012\":[\"20111107\",\"20120311\"],\"2013\":[\"20121109\",\"20130317\"],\"2014\":[\"20131108\",\"20140316\"],\"2015\":[\"20141114\",\"20150315\"],\"2016\":[\"20151113\",\"20160313\"],\"2017\":[\"20161111\",\"20170312\"],\"2018\":[\"20171110\",\"20180311\"],\"2019\":[\"20181106\",\"20190317\"],\"2021\":[\"20201125\",\"20210314\"],\"2022\":[\"20211109\",\"20220313\"],\"2023\":[\"20221107\",\"20230312\"],\"2024\":[\"20231106\",\"20240317\"],\"2025\":[\"20241105\",\"20250317\"]}\narrw=[]\nfrom playwright.async_api import async_playwright\nimport asyncio\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport time\nimport requests\n\nasync def fetch(link):\n    async with async_playwright() as p:\n        browser=await p.chromium.launch(headless=True)\n        page=await browser.new_page()\n        await page.goto(link,wait_until=\"load\")\n        last=None\n        for _ in range(10):\n            await page.wait_for_load_state(\"networkidle\")\n            new=page.url\n            if new==last:\n                break\n            last=new\n        try:\n            await page.wait_for_selector(\"table\")\n        except Exception as e:\n            print(e)\n        html=await page.content()\n        await browser.close()\n        soup=BeautifulSoup(html)\n        return soup\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\n\n\nasync def fetchz(link, context):\n    page = await context.new_page()\n    await page.route(\"**/*\", lambda route: route.abort() if route.request.resource_type in [\"image\", \"stylesheet\", \"font\"] else route.continue_())\n    await page.goto(link, wait_until=\"domcontentloaded\",timeout=0)\n    last = None\n    for _ in range(10):\n        await page.wait_for_load_state(\"networkidle\")\n        new = page.url\n        if new == last:\n            break\n        last = new\n    try:\n        await page.wait_for_selector(\"table\",state=\"attached\")\n    except Exception as e:\n        print(e)\n    element = await page.query_selector(\"table\")\n    html = await element.inner_html()\n    soup = BeautifulSoup(html, \"html.parser\")\n    await page.close()\n    return soup\n\nasync def main(link):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True,args=[\"--disable-gpu\", \"--no-sandbox\"])\n        context = await browser.new_context()\n        result = await fetchz(link,context)\n        await browser.close()\n        return result\nimport nest_asyncio\nnest_asyncio.apply()\narrw=[]\nimport time\nprint(\"done\")\nfor s in seasons:\n    print(s)\n    uk=int(s)-1\n    s=str(s)\n    nj=s.replace(\".0\",\"\")\n    s=s.replace(\".0\",\"\")\n    time.sleep(0.05)\n    d1=s+\"-12-01\"\n    io=float(s)+1\n    io=str(io)\n    io=io.replace(\".0\",\"\")\n    d2=io+\"-01-15\"\n    d3=io+\"-02-15\"\n    dr = [d1,d2,d3,seasons[s][1]]\n    for dt in dr:\n        dt=str(dt)\n        dt=dt.replace(\" 00:00:00\",\"\")\n        url=\"https://barttorvik.com/trank-time-machine.php?date=\"+str(seasons[nj][1])+\"&year=\"+str(s)\n        url=\"https://barttorvik.com/trank.php?year=\"+str(s)+\"&sort=&hteam=&t2value=&conlimit=All&state=All&begin=\"+str(seasons[nj][0])+\"&end=\"+str(dt)+\"&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\"\n        print(url)\n        loop = asyncio.get_event_loop()\n        table = loop.run_until_complete(main(url))\n        for tr in table.find_all(\"tr\"):\n            if \"AdjOE\" not in str(tr) and 'absolute error' not in str(tr):\n                rowz=[]\n                n=0\n                for td in tr.find_all(\"td\"):\n                    if n!=4:\n                        rowz.append(td.text)\n                    else:\n                        fd=td.find_all(\"a\")[0].text\n                        fd=str(fd)\n                        if fd=='':\n                            print(tr.find_all(\"td\"))\n                        rowz.append(fd)\n                    n+=1\n                rowz.append(uk)\n                arrw.append(rowz)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seasons={\"2008\":[\"20071105\",\"20080316\"],\"2009\":[\"20081110\",\"20090310\"],\"2010\":[\"20091109\",\"20100309\"],\"2011\":[\"20101108\",\"20110313\"],\"2012\":[\"20111107\",\"20120311\"],\"2013\":[\"20121109\",\"20130317\"],\"2014\":[\"20131108\",\"20140316\"],\"2015\":[\"20141114\",\"20150315\"],\"2016\":[\"20151113\",\"20160313\"],\"2017\":[\"20161111\",\"20170312\"],\"2018\":[\"20171110\",\"20180311\"],\"2019\":[\"20181106\",\"20190317\"],\"2021\":[\"20201125\",\"20210314\"],\"2022\":[\"20211109\",\"20220313\"],\"2023\":[\"20221107\",\"20230312\"],\"2024\":[\"20231106\",\"20240317\"],\"2025\":[\"20241105\",\"20250317\"]}\narrw=[]\nfrom playwright.async_api import async_playwright\nimport asyncio\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport time\nimport requests\n\nasync def fetch(link):\n    async with async_playwright() as p:\n        browser=await p.chromium.launch(headless=True)\n        page=await browser.new_page()\n        await page.goto(link,wait_until=\"load\")\n        last=None\n        for _ in range(10):\n            await page.wait_for_load_state(\"networkidle\")\n            new=page.url\n            if new==last:\n                break\n            last=new\n        try:\n            await page.wait_for_selector(\"table\")\n        except Exception as e:\n            print(e)\n        html=await page.content()\n        await browser.close()\n        soup=BeautifulSoup(html)\n        return soup\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\n\n\nasync def fetchz(link, context):\n    page = await context.new_page()\n    await page.route(\"**/*\", lambda route: route.abort() if route.request.resource_type in [\"image\", \"stylesheet\", \"font\"] else route.continue_())\n    await page.goto(link, wait_until=\"domcontentloaded\",timeout=60000)\n    last = None\n    for _ in range(10):\n        await page.wait_for_load_state(\"networkidle\")\n        new = page.url\n        if new == last:\n            break\n        last = new\n    try:\n        await page.wait_for_selector(\"table\",state=\"attached\")\n    except Exception as e:\n        print(e)\n    element = await page.query_selector(\"table\")\n    html = await element.inner_html()\n    soup = BeautifulSoup(html, \"html.parser\")\n    await page.close()\n    return soup\n\nasync def main(link):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True,args=[\"--disable-gpu\", \"--no-sandbox\"])\n        context = await browser.new_context()\n        result = await fetchz(link,context)\n        await browser.close()\n        return result\nimport nest_asyncio\nnest_asyncio.apply()\narrw=[]\nimport time\nprint(\"done\")\nfor s in seasons:\n    print(s)\n    uk=int(s)-1\n    s=str(s)\n    nj=s.replace(\".0\",\"\")\n    s=s.replace(\".0\",\"\")\n    time.sleep(0.05)\n    sz=float(s)-1\n    sz=str(sz)\n    sz=sz.replace(\".0\",\"\")\n    d1=sz+\"-12-01\"\n    io=float(s)\n    io=str(io)\n    io=io.replace(\".0\",\"\")\n    d2=io+\"-01-15\"\n    d3=io+\"-02-15\"\n    dr = [d1,d2,d3,seasons[s][1]]\n    ia=0\n    for dt in dr:\n        dt=str(dt)\n        dt=dt.replace(\" 00:00:00\",\"\")\n        url=\"https://barttorvik.com/trank-time-machine.php?date=\"+str(seasons[nj][1])+\"&year=\"+str(s)\n        url=\"https://barttorvik.com/trank.php?year=\"+str(s)+\"&sort=&hteam=&t2value=&conlimit=All&state=All&begin=\"+str(seasons[nj][0])+\"&end=\"+str(dt)+\"&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\"\n        print(url)\n        loop = asyncio.get_event_loop()\n        table = loop.run_until_complete(main(url))\n        for tr in table.find_all(\"tr\"):\n            if \"AdjOE\" not in str(tr) and 'absolute error' not in str(tr):\n                rowz=[]\n                n=0\n                for td in tr.find_all(\"td\"):\n                    if n!=4:\n                        rowz.append(td.text)\n                    else:\n                        fd=td.find_all(\"a\")[0].text\n                        fd=str(fd)\n                        if fd=='':\n                            print(tr.find_all(\"td\"))\n                        rowz.append(fd)\n                    n+=1\n                rowz.append(uk)\n                rowz.append(ia)\n                arrw.append(rowz)\n                ia+=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T23:57:47.284066Z","iopub.execute_input":"2026-01-14T23:57:47.284451Z"}},"outputs":[{"name":"stdout","text":"done\n2008\nhttps://barttorvik.com/trank.php?year=2008&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20071105&end=2007-12-01&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2008&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20071105&end=2008-01-15&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2008&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20071105&end=2008-02-15&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2008&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20071105&end=20080316&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\n2009\nhttps://barttorvik.com/trank.php?year=2009&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20081110&end=2008-12-01&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2009&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20081110&end=2009-01-15&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2009&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20081110&end=2009-02-15&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\nhttps://barttorvik.com/trank.php?year=2009&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20081110&end=20090310&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\n2010\nhttps://barttorvik.com/trank.php?year=2010&sort=&hteam=&t2value=&conlimit=All&state=All&begin=20091109&end=2009-12-01&top=0&revquad=0&quad=5&venue=All&type=All&mingames=0#\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"dfe=pd.DataFrame(arrw)\ndfe.columns=['rk','team','net','kpi','sor','avg','bpi','kp','sag','avgz','q1a','q1','q2','q12','q3','q4','season','unnamed col']\nfor i in range(2018,2026):\n    dfe=dfe[dfe.rk!=i]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T00:37:13.348021Z","iopub.execute_input":"2026-01-03T00:37:13.348416Z","iopub.status.idle":"2026-01-03T00:37:13.386328Z","shell.execute_reply.started":"2026-01-03T00:37:13.348368Z","shell.execute_reply":"2026-01-03T00:37:13.385240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf=pd.DataFrame(arrw)\ndf.columns=['RK','team','conf','G','Rec','ADJOE','ADJDE','BARTHAG','EFG','EFGD','TOR','TORD','ORB','DRB','FTR','FTRD','2P','2PD','3P','3PD','3PR','3PRD','ADJT','WAB','season']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T20:36:05.856560Z","iopub.execute_input":"2025-12-24T20:36:05.856931Z","iopub.status.idle":"2025-12-24T20:36:05.876525Z","shell.execute_reply.started":"2025-12-24T20:36:05.856905Z","shell.execute_reply":"2025-12-24T20:36:05.875139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"goodbart.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T20:39:21.759659Z","iopub.execute_input":"2025-12-24T20:39:21.760137Z","iopub.status.idle":"2025-12-24T20:39:21.826599Z","shell.execute_reply.started":"2025-12-24T20:39:21.760108Z","shell.execute_reply":"2025-12-24T20:39:21.825428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GET ALARGE BIDS","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport pandas as pd\nimport requests\narr=[]\nimport time\nfor i in range(1950,2026):\n    y=i-1\n    print(i)\n    time.sleep(3)\n    url=\"https://www.sports-reference.com/cbb/seasons/men/\"+str(i)+\"-standings.html\"\n    html=requests.get(url).text\n    html=html.replace(\"<!--\",\"\")\n    html=html.replace(\"-->\",\"\")\n    soup=BeautifulSoup(html)\n    \n    for t in soup.find_all(\"table\"):\n        for tr in t.find_all(\"tr\"):\n            if \"Polls\" not in str(tr) and \"Wins\" not in str(tr):\n                tds=tr.find_all(\"td\")\n                school=tds[0].text\n                conf=tds[1].text\n                w=tds[2].text\n                l=tds[3].text\n                pct=tds[4].text\n                wc=tds[6].text\n                lc=tds[7].text\n                pctconf=tds[8].text\n                pf=tds[10].text\n                pa=tds[11].text\n                srs=tds[13].text\n                sos=tds[14].text\n                if(\"NCAA Tournament\" in str(tr)):\n                    if (\"Tournament Champion\" in str(tr)):\n                        m=1\n                    else:\n                        atlarge=1\n                else:\n                    atlarge=0\n                arr.append([int(y)-1,school,conf,w,l,pct,wc,lc,pctconf,pf,pa,srs,sos,atlarge])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T20:45:12.522675Z","iopub.execute_input":"2025-12-24T20:45:12.523123Z","iopub.status.idle":"2025-12-24T20:50:25.914270Z","shell.execute_reply.started":"2025-12-24T20:45:12.523095Z","shell.execute_reply":"2025-12-24T20:50:25.912894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cdf=pd.DataFrame(arr,columns=['Year','Team','Conf','W','L','PCT','CW','CL','CPCT','PF','PA','SRS','SOS','TY'])\nfor row in cdf.itertuples():\n    cdf.at[row.Index,'Year']=float(row.Year+1)\ncdf.to_csv(\"al.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T20:52:32.588860Z","iopub.execute_input":"2025-12-24T20:52:32.589304Z","iopub.status.idle":"2025-12-24T20:52:33.180677Z","shell.execute_reply.started":"2025-12-24T20:52:32.589266Z","shell.execute_reply":"2025-12-24T20:52:33.179523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport yt_dlp\nimport os\n\n# Define the video URL\nvideo_url = 'https://www.youtube.com/watch?v=3lKW4r2W9Pk'  # Replace with the actual video URL\n!yt-dlp --cookies cookies.txt https://www.youtube.com/watch?v=3lKW4r2W9Pk\n# yt-dlp configuration options\nydl_opts = {\n    'format': 'bestvideo+bestaudio/best',  # Selects the best available video and audio quality\n    'merge_output_format': 'mp4',          # Merges the video and audio into an MP4 file\n    'outtmpl': '%(title)s.%(ext)s',        # Output file name template (e.g., \"Video Title.mp4\")\n    'noplaylist': True,                    # Ensures only a single video is downloaded, even if it's part of a playlist\n    'quiet': False   \n    # 'ffmpeg_location': 'C:/path/to/ffmpeg/bin/ffmpeg.exe', # Uncomment and set if FFmpeg is not in PATH\n}\n\ntry:\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([video_url])\n    print(f\"Successfully downloaded: {video_url}\")\nexcept Exception as e:\n    print(f\"An error occurred during download: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T17:13:00.469086Z","iopub.status.idle":"2025-12-23T17:13:00.469416Z","shell.execute_reply":"2025-12-23T17:13:00.469293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bartdf=pd.read_csv(\"https://cfbzzz.alwaysdata.net/bracketology/goodbart.txt\")\naldf=pd.read_csv(\"https://cfbzzz.alwaysdata.net/bracketology/al.txt\")\nfor i in range(2013,2026):\n    print(i)\n    bartdf=bartdf[bartdf.RK!=i]\nbartdf=bartdf.replace('\\xa0', ' ', regex=True)\nbartdf['ty']=False\nfor row in bartdf.itertuples():\n    if \"   \" in str(row.team):\n        bartdf.at[row.Index,'ty']=True\n        teamh=str(row.team).split(\"   \")[0]\n        bartdf.at[row.Index,'team']=teamh\nbartdf['team']=bartdf['team'].str.replace(\" St.\",\" State\")\nbartdf=bartdf.replace(\"Albany\",\"Albany (NY)\")\nbartdf=bartdf.replace(\"BYU\",\"Brigham Young\")\nbartdf=bartdf.replace(\"Grambling State\",\"Grambling\")\nbartdf=bartdf.replace(\"VCU\",\"Virginia Commonwealth\")\nbartdf=bartdf.replace(\"Fairleigh Dickinson\",\"FDU\")\nbartdf=bartdf.replace(\"SIU Edwardsville\",\"Southern Illinois-Edwardsville\")\nbartdf=bartdf.replace(\"LIU\",\"Long Island University\")\nbartdf=bartdf.replace(\"Nebraska Omaha\",\"Omaha\")\nbartdf=bartdf.replace(\"UMBC\",\"Maryland-Baltimore County\")\nbartdf=bartdf.replace(\"Miami FL\",\"Miami (FL)\")\nbartdf=bartdf.replace(\"SMU\",\"Southern Methodist\")\nbartdf=bartdf.replace(\"Penn\",\"Pennsylvania\")\nbartdf=bartdf.replace(\"Arkansas Pine Bluff\",\"Arkansas-Pine Bluff\")\nbartdf=bartdf.replace(\"USC Upstate\",\"South Carolina Upstate\")\nbartdf=bartdf.replace(\"St. Francis NY\",\"St. Francis (NY)\")\nbartdf=bartdf.replace(\"UMKC\",\"Kansas City\")\nbartdf=bartdf.replace(\"Central Connecticut\",\"Central Connecticut State\")\nbartdf=bartdf.replace(\"Tennessee Martin\",\"Tennessee-Martin\")\nbartdf=bartdf.replace(\"Saint Francis\",\"Saint Francis (PA)\")\nbartdf=bartdf.replace(\"Illinois Chicago\",\"Illinois-Chicago\")\nbartdf=bartdf.replace(\"Bethune Cookman\",\"Bethune-Cookman\")\nbartdf=bartdf.replace(\"N.C. State\",\"NC State\")\nbartdf=bartdf.replace(\"Charleston\",\"College of Charleston\")\nbartdf=bartdf.replace(\"Maryland Eastern Shore\",\"Maryland-Eastern Shore\")\nbartdf=bartdf.replace(\"Loyola Chicago\",\"Loyola (IL)\")\nbartdf=bartdf.replace(\"Louisiana Monroe\",\"Louisiana-Monroe\")\nbartdf=bartdf.replace(\"Southern Miss\",\"Southern Mississippi\")\nbartdf=bartdf.replace(\"Texas A&M Corpus Chris\",\"Texas A&M-Corpus Christi\")\nbartdf=bartdf.replace(\"Saint Mary's\",\"Saint Mary's (CA)\")\nbartdf=bartdf.replace(\"UT Rio Grande Valley\",\"Texas-Rio Grande Valley\")\nbartdf=bartdf.replace(\"Gardner Webb\",\"Gardner-Webb\")\nbartdf=bartdf.replace(\"LSU\",\"Louisiana State\")\nbartdf=bartdf.replace(\"Loyola MD\",\"Loyola (MD)\")\nbartdf=bartdf.replace(\"UMass Lowell\",\"Massachusetts-Lowell\")\nbartdf=bartdf.replace(\"UNLV\",\"Nevada-Las Vegas\")\nbartdf=bartdf.replace(\"USC\",\"Southern California\")\nbartdf=bartdf.replace(\"Sam Houston State\",\"Sam Houston\")\nbartdf=bartdf.replace(\"Prairie View A&M\",\"Prairie View\")\nbartdf=bartdf.replace(\"Miami OH\",\"Miami (OH)\")\nbartdf=bartdf.replace(\"VMI\",\"Virginia Military Institute\")\nbartdf=bartdf.replace(\"St. John's\",\"St. John's (NY)\")\nbartdf=bartdf.replace(\"FIU\",\"Florida International\")\nbartdf=bartdf.replace(\"Queens\",\"Queens (NC)\")\nbartdf=bartdf.replace(\"Cal Baptist\",\"California Baptist\")\nbartdf=bartdf.replace(\"Mount State Mary's\",\"Mount St. Mary's\")\nalargez={}\nfor i in range(1949,2026):\n    alargez[float(i)]=[]\nfor row in aldf.itertuples():\n    if float(row.TY)==1:\n        alargez[float(row.Year)].append(row.Team)\nfor row in bartdf.itertuples():\n    if row.team in alargez[float(row.season)]:\n        bartdf.at[row.Index,'alarge']=1\n    elif row.ty==True:\n        bartdf=bartdf.drop(row.Index)\n    else:\n        bartdf.at[row.Index,'alarge']=0\nrow.Unnamed: 0,row.RK,row.team,row.conf,row.G,row.Rec,row.ADJOE,row.ADJDE,row.BARTHAG,row.EFG,row.EFGD,row.TOR,row.TORD,row.ORB,row.DRB,row.FTR,row.FTRD,row.2P,row.2PD,row.3P,row.3PD,row.3PR,row.3PRD,row.ADJT,row.WAB,row.season,row.ty,row.alarge","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T21:40:14.416980Z","iopub.execute_input":"2025-12-24T21:40:14.417440Z","iopub.status.idle":"2025-12-24T21:40:19.781924Z","shell.execute_reply.started":"2025-12-24T21:40:14.417404Z","shell.execute_reply":"2025-12-24T21:40:19.780607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tyz=\"\"\nfor c in bartdf.columns:\n    tyz+=(\"row.\"+c+\",\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T21:43:16.616646Z","iopub.execute_input":"2025-12-24T21:43:16.617025Z","iopub.status.idle":"2025-12-24T21:43:16.622585Z","shell.execute_reply.started":"2025-12-24T21:43:16.616998Z","shell.execute_reply":"2025-12-24T21:43:16.621323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tyz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T21:43:18.063661Z","iopub.execute_input":"2025-12-24T21:43:18.064084Z","iopub.status.idle":"2025-12-24T21:43:18.071383Z","shell.execute_reply.started":"2025-12-24T21:43:18.064050Z","shell.execute_reply":"2025-12-24T21:43:18.069988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for t in set(bartdf.team.tolist()):\n    if t not in set(aldf.Team.tolist()):\n        print('bartdf=bartdf.replace(\"'+t+'\",\"\")')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T21:31:52.776289Z","iopub.execute_input":"2025-12-24T21:31:52.776737Z","iopub.status.idle":"2025-12-24T21:31:52.935279Z","shell.execute_reply.started":"2025-12-24T21:31:52.776709Z","shell.execute_reply":"2025-12-24T21:31:52.934190Z"}},"outputs":[],"execution_count":null}]}